{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Propagation Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Karate Social Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scala compiler version 2.13.5 -- Copyright 2002-2020, LAMP/EPFL and Lightbend, Inc.\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!scalac --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://thibaults-mbp:4040\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1619087686562)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import scala.util.Random\n",
       "import org.apache.spark.graphx.EdgeTriplet\n",
       "import org.apache.spark.graphx.Graph\n",
       "import org.apache.spark.graphx.VertexId\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.graphx.EdgeDirection\n",
       "import org.apache.spark.graphx.Edge\n",
       "import org.apache.spark.sql.Row\n",
       "import java.util.concurrent.atomic.AtomicInteger\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.Random\n",
    "import org.apache.spark.graphx.EdgeTriplet\n",
    "import org.apache.spark.graphx.Graph\n",
    "import org.apache.spark.graphx.VertexId\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.graphx.EdgeDirection\n",
    "import org.apache.spark.graphx.Edge\n",
    "import org.apache.spark.sql.Row\n",
    "import java.util.concurrent.atomic.AtomicInteger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class LPVertex\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Defining Vertex class in order to take into account labels\n",
    "class LPVertex(\n",
    "    var degree : Int = 0,\n",
    "    var initialLabel : Boolean = false,\n",
    "    // string label name is associated with its probability\n",
    "    var vertexLabels : Map[String, Double] = Map[String, Double]() \n",
    ") extends java.io.Serializable {     \n",
    "    \n",
    "    def setDegree(degree: Int) = {\n",
    "        this.degree = degree\n",
    "    }\n",
    "    \n",
    "   // Overriding tostring method\n",
    "    override def toString() : String = {\n",
    "    return \"initialLabel : \" + initialLabel + \"/ vertexLabels = \" + \n",
    "        vertexLabels.foreach { case (key, values) => println(\"key \" + key + \" - \" + values )}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rand: scala.util.Random = scala.util.Random@1e5d773d\n",
       "random_label: (random: scala.util.Random)Int\n",
       "transform: (VD: Int)LPVertex\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Defining function to create half of vertices that are labeled\n",
    "val rand = new Random(4)\n",
    "\n",
    "def random_label(random: Random): Int = {\n",
    "    random.nextInt(3)\n",
    "}\n",
    "\n",
    "def transform(VD: Int): LPVertex = {\n",
    "    val label = random_label(rand);\n",
    "    var v = new LPVertex();\n",
    "\n",
    "    if (rand.nextInt(2) == 1) {\n",
    "        if (label == 0) {\n",
    "            v.initialLabel = true;\n",
    "            v.vertexLabels = Map(\"1\" -> 1.0, \"2\" -> 0.0, \"3\" -> 0.0);\n",
    "        } else if (label == 1) {\n",
    "            v.initialLabel = true;\n",
    "            v.vertexLabels =  Map(\"1\" -> 0.0, \"2\" -> 1.0, \"3\" -> 0.0);\n",
    "        } else if (label == 2) {\n",
    "            v.initialLabel = true;\n",
    "            v.vertexLabels = Map(\"1\" -> 0.0, \"2\" -> 0.0, \"3\" -> 1.0);\n",
    "        }\n",
    "    }\n",
    "    v;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count: java.util.concurrent.atomic.AtomicInteger = 0\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val count = new AtomicInteger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transform2: (VD: Int)LPVertex\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform2(VD: Int): LPVertex = {\n",
    "    var v = new LPVertex();\n",
    "    if (count.get() == 27) {\n",
    "        v.initialLabel = true;\n",
    "        v.vertexLabels = Map(\"1\" -> 1.0, \"2\" -> 0.0);\n",
    "    } else if (count.get() == 17) {\n",
    "        v.initialLabel = true;\n",
    "        v.vertexLabels =  Map(\"1\" -> 0.0, \"2\" -> 1.0);\n",
    "    }\n",
    "    count.incrementAndGet()\n",
    "    v;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VertexProgram: (id: org.apache.spark.graphx.VertexId, VD: LPVertex, m: Map[String,Double])LPVertex\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Vertex Progam for Pregel\n",
    "def VertexProgram(id : VertexId, \n",
    "                  VD : LPVertex, \n",
    "                  m : Map[String, Double]) : LPVertex  = {\n",
    "    var newLPVertex = new LPVertex()\n",
    "    newLPVertex.degree = VD.degree\n",
    "    newLPVertex.initialLabel = VD.initialLabel\n",
    "    newLPVertex.vertexLabels = VD.vertexLabels\n",
    "    \n",
    "    if(VD.initialLabel == false){\n",
    "        newLPVertex.vertexLabels = m.map(x => (x._1, x._2 / VD.degree))\n",
    "    }\n",
    "    newLPVertex\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vertex_count: java.util.concurrent.atomic.AtomicInteger = 0\n",
       "VertexProgram2: (id: org.apache.spark.graphx.VertexId, VD: LPVertex, m: Map[String,Double])LPVertex\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Vertex Progam for Pregel where initial nodes change labels too after certain number of message passing\n",
    "val vertex_count = new AtomicInteger()\n",
    "\n",
    "def VertexProgram2(id : VertexId, \n",
    "                  VD : LPVertex, \n",
    "                  m : Map[String, Double]) : LPVertex  = {\n",
    "    \n",
    "    var newLPVertex = new LPVertex()\n",
    "    vertex_count.incrementAndGet()\n",
    "    newLPVertex.degree = VD.degree\n",
    "    newLPVertex.initialLabel = VD.initialLabel\n",
    "    newLPVertex.vertexLabels = VD.vertexLabels\n",
    "\n",
    "    if(VD.initialLabel == false ){\n",
    "        newLPVertex.vertexLabels = m.map(x => (x._1, x._2 / VD.degree))\n",
    "    }\n",
    "    else if (vertex_count.get() > 10){\n",
    "        newLPVertex.vertexLabels = m.map(x => (x._1, x._2 / VD.degree))\n",
    "    }\n",
    "    newLPVertex\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SendMsg: (triplet: org.apache.spark.graphx.EdgeTriplet[LPVertex,Int])Iterator[(org.apache.spark.graphx.VertexId, Map[String,Double])]\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Send Message\n",
    "def SendMsg(triplet : EdgeTriplet[LPVertex, Int]): Iterator[(VertexId, Map[String, Double])] = {\n",
    "    Iterator((triplet.dstId, triplet.srcAttr.vertexLabels))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MergeMsg: (m1: Map[String,Double], m2: Map[String,Double])Map[String,Double]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Merge Message\n",
    "def MergeMsg(m1 : Map[String, Double],\n",
    "             m2 : Map[String, Double]) : Map[String, Double] = {\n",
    "    val mergeMsg = (m1.keySet ++ m2.keySet).map {i=> (i,m1.getOrElse(i,0.0) + m2.getOrElse(i,0.0))}.toMap\n",
    "    mergeMsg;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "m1: Map[String,Double] = Map(1 -> 1.0, 2 -> 2.0, 3 -> 3.0)\n",
       "m2: Map[String,Double] = Map(1 -> 0.5, 2 -> 0.7, 3 -> 0.3)\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val m1 :Map[String, Double] = Map(\"1\" -> 1, \"2\" -> 2, \"3\" -> 3.0 )\n",
    "val m2 :Map[String, Double] = Map( \"1\"-> 0.5, \"2\"-> 0.7, \"3\" -> 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Map[String,Double] = Map(1 -> 1.5, 2 -> 2.7, 3 -> 3.3)\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MergeMsg(m1, m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Karate Social Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "karate_file: org.apache.spark.rdd.RDD[String] = soc-karate/soc-karate.mtx MapPartitionsRDD[1780] at textFile at <console>:36\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Loading file\n",
    "val karate_file = sc.textFile(\"soc-karate/soc-karate.mtx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "karate_data: org.apache.spark.sql.DataFrame = [value: string]\n",
       "karate_data: org.apache.spark.sql.DataFrame = [value: string]\n",
       "karate_data: org.apache.spark.sql.DataFrame = [value: string]\n",
       "karate_data: org.apache.spark.sql.DataFrame = [value: string]\n",
       "karate_data_splitted: org.apache.spark.sql.DataFrame = [srcId: string, dstID: string]\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var karate_data = karate_file.toDF.withColumn(\"id\",monotonicallyIncreasingId)\n",
    "\n",
    "karate_data = karate_data.withColumn(\"rank\", row_number().over(Window.orderBy(\"id\")))\n",
    "karate_data = karate_data.filter(karate_data(\"rank\")>24)\n",
    "karate_data = karate_data.drop(\"id\",\"rank\")\n",
    "\n",
    "val karate_data_splitted = karate_data.withColumn(\"_tmp\", split($\"value\", \"\\\\ \")).select(\n",
    "  $\"_tmp\".getItem(0).as(\"srcId\"),\n",
    "  $\"_tmp\".getItem(1).as(\"dstID\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rows: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[1790] at rdd at <console>:38\n",
       "karate_edges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]] = MapPartitionsRDD[1791] at map at <console>:39\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rows: RDD[Row] = karate_data_splitted.rdd\n",
    "var karate_edges = rows.map{ case Row(src:String, dist : String) => Edge(src.toLong, dist.toLong, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count: java.util.concurrent.atomic.AtomicInteger = 0\n",
       "initialGraph: org.apache.spark.graphx.Graph[Int,Int] = org.apache.spark.graphx.impl.GraphImpl@145932a1\n",
       "graph: org.apache.spark.graphx.Graph[LPVertex,Int] = org.apache.spark.graphx.impl.GraphImpl@14aef092\n",
       "vertex_rdd: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, LPVertex)] = MapPartitionsRDD[3620] at map at <console>:50\n",
       "edge_rdd: org.apache.spark.graphx.EdgeRDD[Int] = EdgeRDDImpl[3614] at RDD at EdgeRDD.scala:41\n",
       "graph2: org.apache.spark.graphx.Graph[LPVertex,Int] = org.apache.spark.graphx.impl.GraphImpl@667ed400\n",
       "finalGraph: org.apache.spark.graphx.Graph[LPVertex,Int] = org.apache.spark.graphx.impl.GraphImpl@164af661\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Creating graph\n",
    "val count = new AtomicInteger()\n",
    "val initialGraph = Graph.fromEdges(karate_edges, 0)\n",
    "\n",
    "val graph = initialGraph.mapVertices({ case (id, attr) => transform((attr))})\n",
    "\n",
    "val vertex_rdd = graph.degrees.zip(graph.vertices).map({ case (vDeg, vAttr) => vAttr._2.setDegree(vDeg._2) ; vAttr})\n",
    "val edge_rdd = graph.edges\n",
    "\n",
    "val graph2 = Graph(vertex_rdd, edge_rdd)\n",
    "\n",
    "// We have an undirected graph\n",
    "val finalGraph = Graph(graph2.vertices, graph2.edges.union(graph2.edges.reverse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000000000000000000000000000000000(13,2,true,1 -> 1.0 2 -> 0.0 3 -> 0.0)\n",
      "(19,2,false,)\n",
      "(34,17,false,)\n",
      "(5,3,true,1 -> 0.0 2 -> 1.0 3 -> 0.0)\n",
      "(15,2,false,)\n",
      "(4,6,true,1 -> 0.0 2 -> 0.0 3 -> 1.0)\n",
      "(21,2,true,1 -> 0.0 2 -> 0.0 3 -> 1.0)\n",
      "(16,2,false,)\n",
      "(22,2,true,1 -> 0.0 2 -> 1.0 3 -> 0.0)\n",
      "(25,3,false,)\n",
      "(28,4,true,1 -> 0.0 2 -> 0.0 3 -> 1.0)\n",
      "(29,3,true,1 -> 0.0 2 -> 1.0 3 -> 0.0)\n",
      "(11,3,false,)\n",
      "(14,5,false,)\n",
      "(32,6,false,)\n",
      "(30,4,true,1 -> 0.0 2 -> 1.0 3 -> 0.0)\n",
      "(24,5,false,)\n",
      "(27,2,false,)\n",
      "(33,12,false,)\n",
      "(23,2,true,1 -> 1.0 2 -> 0.0 3 -> 0.0)\n",
      "(1,16,true,1 -> 1.0 2 -> 0.0 3 -> 0.0)\n",
      "(6,4,false,)\n",
      "(17,2,false,)\n",
      "(20,3,true,1 -> 0.0 2 -> 0.0 3 -> 1.0)\n",
      "(3,10,false,)\n",
      "(9,5,true,1 -> 0.0 2 -> 0.0 3 -> 1.0)\n",
      "(7,4,true,1 -> 0.0 2 -> 1.0 3 -> 0.0)\n",
      "(12,1,false,)\n",
      "(8,4,false,)\n",
      "(18,2,false,)\n",
      "(31,4,true,1 -> 0.0 2 -> 1.0 3 -> 0.0)\n",
      "(26,3,true,1 -> 0.0 2 -> 1.0 3 -> 0.0)\n",
      "(10,2,true,1 -> 1.0 2 -> 0.0 3 -> 0.0)\n",
      "(2,9,true,1 -> 1.0 2 -> 0.0 3 -> 0.0)\n"
     ]
    }
   ],
   "source": [
    "finalGraph.vertices.foreach(v => println(v._1, v._2.degree, v._2.initialLabel, v._2.vertexLabels.mkString(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initialMsg: scala.collection.immutable.Map[String,Double] = Map(1 -> 0.0, 2 -> 0.0, 3 -> 0.0)\n",
       "maxIter: Int = 100\n",
       "vertex_count: java.util.concurrent.atomic.AtomicInteger = 0\n",
       "graphLP: org.apache.spark.graphx.Graph[LPVertex,Int] = org.apache.spark.graphx.impl.GraphImpl@64da4f06\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val initialMsg = Map(\"1\" -> 0.0, \"2\" -> 0.0, \"3\" -> 0.0 )\n",
    "// val initialMsg = Map(\"1\" -> 0.0, \"2\" -> 0.0)\n",
    "\n",
    "val maxIter = 100\n",
    "val vertex_count = new AtomicInteger()\n",
    "\n",
    "val graphLP = finalGraph.pregel(initialMsg, maxIter, EdgeDirection.Either)(\n",
    "    (id, vert, newLabels) => VertexProgram2(id, vert, newLabels),\n",
    "    it => SendMsg(it),\n",
    "    (a, b) => MergeMsg(a, b)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13,2,true,1 -> 1.0 2 -> 0.0 3 -> 0.0)\n",
      "(19,2,false,1 -> 0.05484161715566786 2 -> 0.2371409567985757 3 -> 0.7065618761905406)\n",
      "(34,17,false,1 -> 0.05597349195396211 2 -> 0.24229817622203798 3 -> 0.7003639525005362)\n",
      "(5,3,true,1 -> 0.0 2 -> 1.0 3 -> 0.0)\n",
      "(15,2,false,1 -> 0.05484161715566786 2 -> 0.2371409567985757 3 -> 0.7065618761905406)\n",
      "(4,6,true,1 -> 0.0 2 -> 0.0 3 -> 1.0)\n",
      "(21,2,true,1 -> 0.0 2 -> 0.0 3 -> 1.0)\n",
      "(16,2,false,1 -> 0.05484161715566786 2 -> 0.2371409567985757 3 -> 0.7065618761905406)\n",
      "(22,2,true,1 -> 0.0 2 -> 1.0 3 -> 0.0)\n",
      "(25,3,false,1 -> 0.06397607964859292 2 -> 0.26673906232994343 3 -> 0.667648960717103)\n",
      "(28,4,true,1 -> 0.05914445589493983 2 -> 0.25340422325476836 3 -> 0.6859519111495134)\n",
      "(29,3,true,1 -> 0.06139097665400267 2 -> 0.2607097769124235 3 -> 0.6765420049317887)\n",
      "(11,3,false,1 -> 0.06599430422450676 2 -> 0.674207943538445 3 -> 0.2593688475904055)\n",
      "(14,5,false,1 -> 0.05773038300977386 2 -> 0.2562729805520182 3 -> 0.6851968943900304)\n",
      "(32,6,false,1 -> 0.06920401721870725 2 -> 0.2818566090892987 3 -> 0.6475099496397102)\n",
      "(30,4,true,1 -> 0.05579721821296588 2 -> 0.24056559467773422 3 -> 0.7020631824959905)\n",
      "(24,5,false,1 -> 0.057640806807728415 2 -> 0.24664104038453522 3 -> 0.6941234753851964)\n",
      "(27,2,false,1 -> 0.055878849399729466 2 -> 0.24140257638548596 3 -> 0.7011476130187219)\n",
      "(33,12,false,1 -> 0.053721797839128484 2 -> 0.23203804909784054 3 -> 0.7128820180911393)\n",
      "(23,2,true,1 -> 0.05484161715566786 2 -> 0.2371409567985757 3 -> 0.7065618761905406)\n",
      "(1,16,true,1 -> 0.1165971848808997 2 -> 0.42446668426948914 3 -> 0.4582917715708243)\n",
      "(6,4,false,1 -> 0.08139105634815155 2 -> 0.5981811522741748 3 -> 0.31986879197980356)\n",
      "(17,2,false,1 -> 0.07478950080174909 2 -> 0.6307507839235822 3 -> 0.2939103627606568)\n",
      "(20,3,true,1 -> 0.07654804103493089 2 -> 0.3411115411526993 3 -> 0.5813689802662124)\n",
      "(3,10,false,1 -> 0.05901228266470297 2 -> 0.25805051066051593 3 -> 0.6819230579790796)\n",
      "(9,5,true,1 -> 0.0688357740298902 2 -> 0.28725436043344005 3 -> 0.6427076257698253)\n",
      "(7,4,true,1 -> 0.06819249522607473 2 -> 0.663340913862003 3 -> 0.2679980610451601)\n",
      "(12,1,false,1 -> 0.11659433161340658 2 -> 0.4244538298787462 3 -> 0.4582628452073611)\n",
      "(8,4,false,1 -> 0.058171116165248796 2 -> 0.2597734861710072 3 -> 0.6814204421786487)\n",
      "(18,2,false,1 -> 0.08683833635845928 2 -> 0.390531832691018 3 -> 0.5219021187815401)\n",
      "(31,4,true,1 -> 0.05889900646095721 2 -> 0.27953053148172186 3 -> 0.6603296999892383)\n",
      "(26,3,true,1 -> 0.0636000896259553 2 -> 0.26504791633341135 3 -> 0.6696910635397589)\n",
      "(10,2,true,1 -> 0.05748762116597536 2 -> 0.25015061868902744 3 -> 0.6910901171935357)\n",
      "(2,9,true,1 -> 0.057085515057108224 2 -> 0.35662413463220477 3 -> 0.5855735698278997)\n"
     ]
    }
   ],
   "source": [
    "graphLP.vertices.foreach(v => println(v._1, v._2.degree, v._2.initialLabel, v._2.vertexLabels.mkString(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "m: org.apache.spark.rdd.RDD[(Long, Long)] = MapPartitionsRDD[3554] at map at <console>:37\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Get label estimate after algorithm\n",
    "val m = graphLP.vertices.map({case(k,v) => (k.toLong, v.vertexLabels.maxBy{ case (key, value) => value }._1.toLong)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "//saving labels in file\n",
    "// m.coalesce(1).saveAsTextFile(\"karate_labels\")\n",
    "// m.coalesce(1).saveAsTextFile(\"karate_2_labels\")\n",
    "m.coalesce(1).saveAsTextFile(\"karate_vertex_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
